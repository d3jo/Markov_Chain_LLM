# LLM using Markov Chain algorithm

Built a probabilistic text-generation model that learns next-word predictions from frequency patterns in a book,
demonstrating the core principle behind modern transformer models. Whereas my model estimated probabilities of word sequences locally, 
transformers generalize this idea with self-attention to capture relationships across an entire text, enabling context-aware and coherent language generation.
